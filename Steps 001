# Basic Steps:

---load---
val df = spark.read.json("/home/rg/Project/eno.json")

---Drop---
val df2 = df1.na.drop()

---Seperate---

val df1 =  df.withColumn("Client_entropy",col("session.client_entropy"))
val df2 = df1.withColumn("Server_entropy",col("session.server_entropy")).withColumn("ports",col("transport.ports"))
val df3 =  df2.withColumn("protocol",col("transport.protocol")).withColumn("ssl_client_cipher_suites",col("payload.ssl_client_cipher_suites"))
val df3 =  df2.withColumn("network",col("network")).withColumn("has_content",col("has_content"))
val df4 =  df3.withColumn("arp_headers",col("payload.arp_headers")).withColumn("client_average_interarrival_time",col("session.client_average_interarrival_time"))

val df8 = df7.drop("datalink")
df10.write.json("/home/rg/Project/sample001.json")
val df4 = df3.withColumn("client_server",struct((col("server_entropy")),(col("client_average_interarrival_time"))))
val df3 = df2.withColumn("label", lit("0"))
import org.apache.spark.ml.feature.FeatureHasher
import org.apache.spark.ml.PipelineStage
import org.apache.spark.ml.Transformer
val hasher = new FeatureHasher().setInputCols("Client_entropy", "Server_entropy").setOutputCol("features")
val featurized = hasher.transform(df2)
featurized.show()
val df3 = featurized.withColumn("label", lit(0.0)).drop("Client_entropy", "Server_entropy")
import org.apache.spark.mllib.util.MLUtils
val convertedVecDF = MLUtils.convertVectorColumnsToML(df2)
convertedVecDF.write.format("libsvm").save("/home/rg/Project/checkthisout")

# To create label & feature:

val df = spark.read.json("/home/rg/Project/sample005.json")
import org.apache.spark.ml.feature.FeatureHasher
import org.apache.spark.ml.PipelineStage
import org.apache.spark.ml.Transformer
import org.apache.spark.ml.feature.StringIndexer
val indexer = new StringIndexer().setInputCol("src_ip").setOutputCol("label")
val indexed = indexer.fit(df).transform(df)

val hasher = new FeatureHasher().setInputCols("client_entropy", "src_ip", "client_average_interarrival_time", "client_empty_pkt_count", "client_max_pkt_size", "client_min_pkt_size", "client_nonempty_packet_count", "client_std_dev_interarrival_time", "src_os_fingerprint", "client_ip_class_of_service").setOutputCol("features")

val featurized = hasher.transform(indexed)

val df1 = featurized.drop("client_average_interarrival_time","client_empty_pkt_count","client_entropy","client_ip_class_of_service","client_max_pkt_size","client_min_pkt_size","client_nonempty_packet_count","client_std_dev_interarrival_time", "src_ip", "src_os_fingerprint")

import org.apache.spark.mllib.util.MLUtils
val convertedVecDF = MLUtils.convertVectorColumnsToML(df1)
convertedVecDF.write.format("libsvm").save("/home/rg/Project/final001")


# For ML Data: (Uni/Bi/Flow)direction

val df = spark.read.json("/home/ritamg/Project/ml")

val df1 = df.withColumn("client_bi_dir_pkt_arrival_gaps",col("session.client_bi_dir_pkt_arrival_gaps")).withColumn("src_ip",col("network.src_ip"))
val df2 = df1.drop("datalink").drop("session").drop("payload").drop("transport").drop("session").drop("network").drop("has_content").drop("capture_filters").drop("client_uni_dir_ml_packet_arrival_gaps")



# For ML Data: (bi direc/src ip)

val df = spark.read.json("/home/ritamg/Project/final001")
val df1 = df.withColumn("min", array_min(col("client_bi_dir_pkt_arrival_gaps"))).withColumn("max", array_max(col("client_bi_dir_pkt_arrival_gaps")))

val min_max = df1.agg(min("min"), max("max")).head()
val global_min = min_max(0)
val global_max = min_max(1)

val df2 = df1.drop("min", "max")
val df3 = df2.withColumn("min", lit(global_min)).withColumn("max", lit(global_max))

def bucketing_udf(gap_seq: Seq[Double], min: Double, max:Double): Seq[Int]={
	val no_bins =300
	val width_temp = ((max - min )/ no_bins)
	val width = math.ceil(width_temp).toInt
	var freq_arr = Array.fill[Int](no_bins)(0)
	for(j <- 0 to gap_seq.size-1){
		freq_arr(  ((gap_seq(j)-min)/width).toInt ) = freq_arr(  ((gap_seq(j)-min)/width).toInt )  + 1
	}
	freq_arr
}



val generate_bucketing = udf((gap_seq: Seq[Double], min: Double, max: Double) => bucketing_udf(gap_seq, min, max))
val df4 = df3.withColumn("feature", generate_bucketing(col("client_bi_dir_pkt_arrival_gaps"), col("min"), col("max")  ))




















